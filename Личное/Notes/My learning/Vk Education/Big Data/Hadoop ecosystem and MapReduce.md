# План занятия

- [[Hadoop ecosystem and MapReduce#Концепция MapReduce|Концепция MapReduce]]
- [[Hadoop ecosystem and MapReduce#Реализация MapReduce|Реализация MapReduce]]
- [Менеджер ресурсов YARN]
- [MapReduce API и Hadoop Streaming]

## Концепция MapReduce

### Экосистема Hadoop

![[Pasted image 20250125235103.png]]

Немного вспомним как устроена архитектуру [[Introduction in Big Data#Архитектура HDFS|HDFS]]. Также следует вспомнить [[Introduction in Big Data#Hadoop. Locality & Distances|Hadoop. Locality & Distances]], ну и чтение/запись. 

Давайте представим следующую задачу, что нам нужно скопировать данные из одной папки в другую внутри RDBMS. Нам надо продумать, куда и как мы положим копии наших данных.

#### DistCp vs Cp in Real Life

![[Pasted image 20250126001533.png]]

Слева у нас консольный клиент и как он работает. Разбит на кусочки из-за большого объема данных для обработки (ну скорее всего, так предполагается). Куда мы копируем, предположим, что это наш ноутбук, где мы будем работать с данными, то он становится bottleneck-ом. Из-за того, что большие данные, сеть и как будут работать наш диск и т.д.
Несмотря на все эти параметры - все будет единичная нода и будет она довольно долго выполняться. Для это можно использоваться распределенная обработка, несмотря на то, что у нас тут так таковой обработки нету, но она нам тут пригодится. Для подобных нужд нужно много нод для обработки кусочков. Вот какого типа будет обработка, будет определять насколько вычислительно и архитектурно сложной будет построена цепочка этого расчета.
Концепция того, как она будет устроена и на какие этапы будет разбита - об этом будет идти речь (MapReduce).

#### MapReduce. Определения

Хочется сказать, что не стоит путать MapReduce и реализацию MapReduce, она используется не только в MapReduce. Spark, на самом деле работает на MapReduce. Эти концепции применимы почти ко всем современным фреймворкам распределенных обработчиков данных.

- __MapReduce__ - модель распределенных вычислений. Реализация Hadoop MapReduce на Java. Сегодня напрямую используется редко, в основном это поддержка легаси.
- __Map__ - обработка данных -> Mapper
- __Reduce__ - свертка данных -> Reducer
- __Split__ - независимая (!!!) часть для обработки
- Работаем с __key-value__ данными

Достаточно просто расписать его в каком-то понятном SQL-line синтаксисе или Pandas/Numpy, если работали с ним - для понимания использования практически нашей концепции.

#### MapReduce. No Reduce

![[Pasted image 20250126002650.png]]

**Зачем это может быть нужно**
1. Просто скопировать данные
2. Отфильтровать данные
3. Сконвертировать данные

У нас есть датасет с пользователями и действиями, которые они совершали на портале. Нам нужно отфильтровать данные по определенному действию. Для последующей аналитики и их обработки. 

> [!tip] __Ключевой идея__ 
> Что это __No Reduce__, у нас отсутствует No Reduce компонент - это при операции и её реализации, __нет необходимости в обмене данными__ между сплитами (либо результатами между сплитами).

В будущем, когда мы будем говорить о алгоритме распределенной обработки, который мы хотим реализовать. Он должен состоять, как можно из большего числа простых кусочков.

#### MapReduce. Концепт

Более сложные варианты для действительно практических задач, которые мы не можем впихнуть в эту парадигму предполагают, что нам на этапе Reduce или на этапе свертки, нужно будет поделиться либо со всеми, либо с частью других мапперов, других сплитов. Эта операция называется __shuffle__. Мы должны избегать shuffle-ов, потому что это самое дорогое место в нашей оптимизации.

![[Pasted image 20250126004057.png]]
 

## Реализация MapReduce

В реальной жизни у нас есть Hadoop MapReduce в котором есть несколько готовых для использования классов, которые надо будет использовать или писать самим.

#### MapReduce. Input Data

- Класс __InputFormat__ ответственен за разбиение входных данных на сплиты
- Размер сплита считается по формуле $max(minSize, min(maxSize, blockSize))$ 
	maxSize -> mapred.max.split.size
	minSize -> mapred.min.split.size
	blockSize -> HDFS block size

![[Pasted image 20250126004718.png]]

![[Pasted image 20250126004737.png]]

В реальности, мы должны указывать размер сплита такой, сколько мы готовы выделить ресурсов мапперов, которые будут готовы считать и обрабатывать. Сколько мы хотим, чтобы эта таска обрабатывалась. Сколько мы хотим выходных кусочков.

В распределенных вычислениях встречаются отказы. 

Такой вот наборчик входных данных он поддерживает.

![[Pasted image 20250126005351.png]]

#### MapReduce. Map Phase

![[Pasted image 20250126005514.png]]

1. При этом есть такое правило, что важно уметь двигать код к данным (data locality). На самом деле она осталась в концепции, но YARN может оптимально распределить ближе к нашему мапперу. Но в современных реалиях, где данные могут хранится в удаленных хранилищах S3, то там уже по факту Data Locality не учитывается, хотя концепция этого требует.

2. На самом деле он вытекает из самой концепции MapReduce. Если мы посмотрим, то на шаге Map, нужно подобрать так, чтобы код правильно подъехал. И на шаге Reduce, там уже будет без разницы, относительно, им просто надо будет взять данные. Но вот важно будет, чтобы shuffle база была оптимизирована тоже хорошо, с Locality, уже не влияет.


#### MapReduce. Map

1. Способен получать данные из различных источников. По умолчанию встроены реализация чтения из текстовых файлов (ключ от значения отделен табуляцией) и SequenceFile (ключ и значение хранятся в сериализованном виде). Можно написать собственные входные форматы данных. Например:
	1. Чтение из нескольких директорий
	2. Разбиение файла на части и чтение только своей части
	3. Чтение JSON
	4. Чтение из Kafka
2. Количество мапперов равно количеству входных сущностей (сплитов) согласно входному формату данных.

#### MapReduce. Mapper Results and Shuffle

- Итоговые данные работы маппера - это key-value. Они записываются на локальный диск ноды.
- Для каждого редюсера маппер готовит свой файл для шаффла по ключу
- Данные с одним ключом попадают на один редюсер

![[Pasted image 20250126171148.png]]

